{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42069\n"
     ]
    },
    {
     "data": {
      "text/plain": "42069"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "\n",
    "pl.seed_everything(420_69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'next_state', 'reward', 'next_valid_actions', 'is_terminal'))\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, capacity: int = 10_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def push(self, experience: Experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def get_batched_experience(self, batch_size: int):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, next_state, reward, next_valid_actions, is_terminal = zip(*(self.buffer[idx] for idx in indices))\n",
    "\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(next_state),\n",
    "            np.array(reward),\n",
    "            np.array(next_valid_actions),\n",
    "            np.array(is_terminal),\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "\n",
    "class ExperienceDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, replay_buffer: ReplayBuffer, sample_size: int = 200):\n",
    "        self.buffer = replay_buffer\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        states, actions, next_state, reward, next_valid_actions = self.buffer.sample(self.sample_size)\n",
    "        for idx in range(len(states)):\n",
    "            yield states[idx], actions[idx], next_state[idx], reward[idx], next_valid_actions[idx]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "from functools import reduce\n",
    "import random\n",
    "\n",
    "from src.environment import Environment\n",
    "from src.agent import Agent\n",
    "\n",
    "class DeepQLearningAgent(Agent):\n",
    "\n",
    "    def __init__(self, env: Environment, replay_buffer: ReplayBuffer):\n",
    "        super().__init__(env)\n",
    "        self._replay_buffer = replay_buffer\n",
    "        self.state, _, self.valid_actions, self.is_terminal = self._env.set_to_initial_state()\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        shape = self._env.observation_shape\n",
    "        if isinstance(shape, tuple):\n",
    "            return reduce(lambda x, y: x * y, shape)\n",
    "        else:\n",
    "            return shape\n",
    "\n",
    "    @property\n",
    "    def n_actions(self):\n",
    "        return self._env.num_actions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def best_action(self, state: torch.tensor, valid_actions: set[int], net: torch.nn.Module, eps: float,\n",
    "                    device):\n",
    "        valid_actions = list(valid_actions)\n",
    "\n",
    "        if random.uniform(0, 1) < eps:\n",
    "            valid_actions.sort()\n",
    "\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "            q_vec = net(state)\n",
    "            _, action = torch.max(q_vec, dim=1)  # TODO should only pick from valid actions\n",
    "            action = int(action.item())\n",
    "        else:\n",
    "            action = random.choice(valid_actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def perform_step(self, net: torch.nn.Module, eps: float, device):\n",
    "        # Get next action\n",
    "        action = self.best_action(self.state, self.valid_actions, net, eps)\n",
    "\n",
    "        # Step\n",
    "        new_state, reward, new_valid_actions, is_terminal = self._env.act(action)\n",
    "        experience = Experience(\n",
    "            state=self.state,\n",
    "            action=action,\n",
    "            next_state=new_state,\n",
    "            reward=reward,\n",
    "            next_valid_actions=new_valid_actions,\n",
    "            is_terminal=is_terminal,\n",
    "        )\n",
    "\n",
    "        self._replay_buffer.push(experience)\n",
    "\n",
    "        self.state = new_state\n",
    "        if is_terminal:\n",
    "            self.state, _, self.valid_actions, self.is_terminal = self._env.set_to_initial_state()\n",
    "        else:\n",
    "            self.valid_actions = new_valid_actions\n",
    "\n",
    "        return reward, is_terminal\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class DQN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, n_actions: int, hidden_size: int = 256, activation_fn=torch.nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.input_layer = torch.nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        self.output_layer = torch.nn.Linear(in_features=hidden_size, out_features=n_actions)\n",
    "        self.activation = activation_fn()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.input_layer(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DQNConfig:\n",
    "    batch_size: int = 16,\n",
    "    lr: float = 5e-5\n",
    "    replay_size: int = 1000\n",
    "    init_buffer_steps: int = 1000\n",
    "    eps_init: float = 1.0\n",
    "    eps_final: float = .05\n",
    "    eps_decay_timesteps: int = 25_000\n",
    "    beta: float = .5\n",
    "    gamma: float = .99\n",
    "    sync_rate: int = 10\n",
    "\n",
    "\n",
    "class DQNLightning(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, env: Environment, config: DQNConfig, device, optimizer_fn=torch.optim.AdamW):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.config = config\n",
    "        self._replay_buffer = ReplayBuffer(config.replay_size)\n",
    "        self.device = device\n",
    "\n",
    "        self._agent = DeepQLearningAgent(\n",
    "            env=env,\n",
    "            replay_buffer=self._replay_buffer\n",
    "        )\n",
    "\n",
    "        # Candidate and target nets\n",
    "        self.net = DQN(input_size=self._agent.observation_space, n_actions=self._agent.n_actions)\n",
    "        self.target_net = DQN(input_size=self._agent.observation_space, n_actions=self._agent.n_actions)\n",
    "\n",
    "        self._timesteps = 0\n",
    "        self._total_reward = 0\n",
    "        self._episode_reward = 0\n",
    "\n",
    "        self._eps = self.config.eps_init\n",
    "        self._k = (config.eps_init - config.eps_final) / config.eps_decay_timesteps\n",
    "        self._optimizer_fn = optimizer_fn\n",
    "\n",
    "        self.init_replay_buffer(steps=config.init_buffer_steps, device=self.device)\n",
    "\n",
    "    @property\n",
    "    def eps(self):\n",
    "        if self._timesteps < self.config.eps_decay_timesteps:\n",
    "            return self.config.eps_init - self._k * self._timesteps\n",
    "\n",
    "        return self.config.eps_final\n",
    "\n",
    "    def init_replay_buffer(self, steps: int, device):\n",
    "        for _ in range(steps):\n",
    "            self._agent.perform_step(self.net, eps=1.0, device=device)\n",
    "\n",
    "    def compute_loss(self, batch):\n",
    "        states, actions, next_states, rewards, next_valid_actions, are_terminal = batch\n",
    "        state_action_values = self.net(states).gather(1, actions.long().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_states).max(1)[0]\n",
    "            next_state_values[are_terminal] = 0.0\n",
    "            next_state_values = next_state_values.detach()\n",
    "\n",
    "        expected_state_action_values = next_state_values * self.config.gamma + rewards\n",
    "\n",
    "        return torch.nn.functional.mse_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    def training_step(self, batch, batch_idx: int):\n",
    "        eps: float = self.eps\n",
    "\n",
    "        reward, is_terminal = self._agent.perform_step(self.net, eps=eps, device=self.device)\n",
    "        self._episode_reward += reward\n",
    "        self.log('episode_reward', self._episode_reward)\n",
    "\n",
    "        loss = self.compute_loss(batch)\n",
    "        if is_terminal:\n",
    "            self._total_reward = self._episode_reward\n",
    "            self._episode_reward = 0\n",
    "\n",
    "        self.log('eps', eps)\n",
    "\n",
    "        self._timesteps += 1\n",
    "\n",
    "        if self._timesteps % self.config.sync_rate == 0:\n",
    "            self.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "        self.log_dict({\n",
    "            'train_loss': loss,\n",
    "            'reward': self._total_reward,\n",
    "        })\n",
    "\n",
    "        self.log('total_reward', self._total_reward, prog_bar=False)\n",
    "        self.log('timesteps', self._timesteps, prog_bar=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self._optimizer_fn(self.net.parameters(), lr=self.config.lr)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._dataloader()\n",
    "\n",
    "    def _dataloader(self):\n",
    "        dataset = ExperienceDataset(self._replay_buffer)\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "        )\n",
    "\n",
    "        return dataloader\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "perform_step() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_25720\\3491525995.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m model = DQNLightning(\n\u001B[0m\u001B[0;32m      7\u001B[0m     \u001B[0menv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0menvironment\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mDQNConfig\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_25720\\3558238859.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, env, config, optimizer_fn)\u001B[0m\n\u001B[0;32m     57\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_optimizer_fn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moptimizer_fn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 59\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minit_replay_buffer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msteps\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minit_buffer_steps\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     60\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     61\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_25720\\3558238859.py\u001B[0m in \u001B[0;36minit_replay_buffer\u001B[1;34m(self, steps)\u001B[0m\n\u001B[0;32m     68\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0minit_replay_buffer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msteps\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msteps\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 70\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_agent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mperform_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meps\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     71\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mcompute_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     25\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclone\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mF\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: perform_step() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.environment import CartPole\n",
    "\n",
    "environment = CartPole()\n",
    "\n",
    "\n",
    "model = DQNLightning(\n",
    "    env=environment,\n",
    "    config=DQNConfig(),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
